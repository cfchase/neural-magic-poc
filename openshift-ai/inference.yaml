---
# apiVersion: serving.kserve.io/v1beta1
# kind: InferenceService
# metadata:
#   annotations:
#     serving.knative.openshift.io/enablePassthrough: "true"
#     sidecar.istio.io/inject: "true"
#     sidecar.istio.io/rewriteAppHTTPProbers: "true"
#   name: neural-magic-llm-poc
# spec:
#   predictor:
#     serviceAccountName: sa-s3
#     containers:
#     - args:
#         - --integration
#         - openai
#         - --config-file
#         - /server-config.yaml
#         - --port
#         - "8080"
#       command:
#         - deepsparse.server
#       env:
#         - name: STORAGE_URI
#           value: s3://models/models
#       name: kserve-container
#       image: quay.io/ltomasbo/neural-magic:llm
#       ports:
#       - containerPort: 8080
#       resources:
#         limits:
#           cpu: '2'
#           memory: '24Gi'
#         requests:
#           cpu: '2'
#           memory: '24Gi'

## OPTION B to workaround the problem with the read only volumen
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
  name: neural-magic-llm-poc
spec:
  predictor:
    volumes:
      - name: kserve-aux
        emptyDir: {}
    serviceAccountName: sa-s3
    containers:
    - command: ["/bin/sh", "-c"]
      args:
      - |
        cp -r /mnt/models/deployment/* /mnt/models-aux &&
        deepsparse.server --integration openai --config-file /server-config.yaml --port 8080
      env:
        - name: STORAGE_URI
          value: s3://models/models
      name: kserve-container
      image: quay.io/ltomasbo/neural-magic:llm-aux
      ports:
      - containerPort: 8080
      resources:
        limits:
          cpu: '2'
          memory: '24Gi'
        requests:
          cpu: '2'
          memory: '24Gi'
      volumeMounts:
        - name: kserve-aux
          readOnly: false
          mountPath: /mnt/models-aux
