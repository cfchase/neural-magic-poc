apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: llm-pruning-pipeline
  annotations:
    tekton.dev/output_artifacts: '{}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"download-model": [], "eval-model": [], "eval-model-2":
      [], "eval-model-3": [], "eval-model-4": [], "eval-model-5": [], "eval-model-6":
      [], "eval-model-7": [], "eval-model-8": [], "eval-model-9": [], "export-model":
      [], "export-model-2": [], "export-model-3": [], "export-model-4": [], "quantize-cpu-model":
      [], "quantize-cpu-model-2": [], "quantize-gpu-model": [], "quantize-gpu-model-2":
      [], "sparse-model": [], "upload-model": [], "upload-model-2": [], "upload-model-3":
      [], "upload-model-4": [], "upload-model-5": [], "upload-model-6": [], "upload-model-7":
      [], "upload-model-8": []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A Pipeline for pruning
      LLMs with SparseML", "inputs": [{"default": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "name": "model_name", "optional": true, "type": "String"}, {"default": "CPU",
      "name": "inference_target", "optional": true, "type": "String"}, {"default":
      "HF", "name": "download_option", "optional": true, "type": "String"}, {"default":
      "models-shared", "name": "shared_volume", "optional": true, "type": "String"},
      {"default": "True", "name": "sparse", "optional": true, "type": "Boolean"},
      {"default": "0.5", "name": "sparsity_ratio", "optional": true, "type": "Float"},
      {"default": "True", "name": "quantize", "optional": true, "type": "Boolean"},
      {"default": "False", "name": "eval", "optional": true, "type": "Boolean"}, {"default":
      "hellaswag", "name": "eval_task", "optional": true, "type": "String"}, {"default":
      "64", "name": "eval_batch_size", "optional": true, "type": "String"}, {"default":
      "True", "name": "save_model", "optional": true, "type": "Boolean"}, {"default":
      "optimized-1", "name": "save_folder_name", "optional": true, "type": "String"}],
      "name": "LLM Pruning Pipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: download_option
    value: HF
  - name: eval
    value: "False"
  - name: eval_batch_size
    value: '64'
  - name: eval_task
    value: hellaswag
  - name: inference_target
    value: CPU
  - name: model_name
    value: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  - name: quantize
    value: "True"
  - name: save_folder_name
    value: optimized-1
  - name: save_model
    value: "True"
  - name: shared_volume
    value: models-shared
  - name: sparse
    value: "True"
  - name: sparsity_ratio
    value: '0.5'
  pipelineSpec:
    params:
    - name: download_option
      default: HF
    - name: eval
      default: "False"
    - name: eval_batch_size
      default: '64'
    - name: eval_task
      default: hellaswag
    - name: inference_target
      default: CPU
    - name: model_name
      default: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    - name: quantize
      default: "True"
    - name: save_folder_name
      default: optimized-1
    - name: save_model
      default: "True"
    - name: shared_volume
      default: models-shared
    - name: sparse
      default: "True"
    - name: sparsity_ratio
      default: '0.5'
    tasks:
    - name: download-model
      params:
      - name: download_option
        value: $(params.download_option)
      - name: model_name
        value: $(params.model_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-name
          - $(inputs.params.model_name)
          - --destination-path
          - /mnt/models/llm
          - --download-option
          - $(inputs.params.download_option)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'huggingface-hub' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
            pip install --quiet --no-warn-script-location 'huggingface-hub' 'boto3'
            --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def download_model(model_name, destination_path,
                               download_option):
                if download_option == "HF":
                    import subprocess
                    print('Starting downloading the model from HF')
                    # Execute the huggingface_hub-cli command
                    result = subprocess.run(["huggingface-cli", "download", model_name,
                                             "--local-dir", destination_path,
                                             "--local-dir-use-symlinks", "False"],
                                            capture_output=True, text=True)
                    # Check for errors or output
                    if result.returncode == 0:
                        print("Model downloaded successfully from HF.")
                    else:
                        print("Error downloading model:")
                        print(result.stderr)

                elif download_option == "S3":
                    import os
                    import errno
                    from boto3 import client

                    print('Starting downloading the model from S3')

                    s3_endpoint_url = os.environ["s3_host"]
                    s3_access_key = os.environ["s3_access_key"]
                    s3_secret_key = os.environ["s3_secret_access_key"]
                    s3_bucket_name = os.environ["s3_bucket"]

                    s3_client = client(
                        's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                        aws_secret_access_key=s3_secret_key, verify=False
                    )

                    # list all objects in the folder
                    objects = s3_client.list_objects(Bucket=s3_bucket_name, Prefix=model_name)

                    # download each object in the folder
                    for object in objects['Contents']:
                        file_name = object['Key']
                        local_file_name = os.path.join(destination_path, file_name.replace(model_name, '')[1:])
                        if not os.path.exists(os.path.dirname(local_file_name)):
                            try:
                                os.makedirs(os.path.dirname(local_file_name))
                            except OSError as exc: # Guard against race condition
                                if exc.errno != errno.EEXIST:
                                    print("Error downloading model")
                                    raise
                        print('Download file to')
                        print(file_name)
                        print(local_file_name)
                        s3_client.download_file(s3_bucket_name, file_name, local_file_name)

                    print('Model downloaded successfully from S3.')

                elif download_option == "PVC":
                    print('Model should be already on the volumen.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Download model', description='')
            _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--destination-path", dest="destination_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--download-option", dest="download_option", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = download_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: download_option
        - name: model_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Download model",
              "outputs": [], "version": "Download model@sha256=b7fbdb69001d09507ffd79158bada03ecad235fad1143f8b338b2bf7928c542b"}'
    - name: sparse-model
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      - name: sparsity_ratio
        value: $(params.sparsity_ratio)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --compress-model-path
          - /mnt/models/sparse-llm
          - --ds
          - open_platypus
          - --sparsity-ratio
          - $(inputs.params.sparsity_ratio)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'datasets' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def sparse_model(model_path, compress_model_path, ds,
                             sparsity_ratio):
                import sparseml.transformers

                recipe = f"""
                test_stage:
                  obcq_modifiers:
                    SparseGPTModifier:
                      sparsity: {sparsity_ratio}
                      sequential_update: true
                      targets: ["re:model.layers.\\\d*$"]
                """

                sparseml.transformers.oneshot(
                    model=model_path,
                    dataset=ds,
                    recipe=recipe,
                    output_dir=compress_model_path,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Sparse model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--compress-model-path", dest="compress_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--ds", dest="ds", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--sparsity-ratio", dest="sparsity_ratio", type=float, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = sparse_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        - name: sparsity_ratio
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Sparse model",
              "outputs": [], "version": "Sparse model@sha256=924ebd9aa551e8f122e4ff6e982a7d38ddca97ea9e006a1ae3af8daf7dce14c1"}'
      when:
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: quantize-cpu-model
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --compress-model-path
          - /mnt/models/quant-llm
          - --ds
          - open_platypus
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'datasets' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def quantize_cpu_model(model_path, compress_model_path, ds):
                import sparseml.transformers

                recipe = """
                test_stage:
                  obcq_modifiers:
                    LogarithmicEqualizationModifier:
                      mappings: [
                        [["re:.*q_proj", "re:.*k_proj", "re:.*v_proj"], "re:.*input_layernorm"],
                        [["re:.*gate_proj", "re:.*up_proj"], "re:.*post_attention_layernorm"],
                      ]
                    QuantizationModifier:
                      ignore:
                        # These operations don't make sense to quantize
                        - LlamaRotaryEmbedding
                        - LlamaRMSNorm
                        - SiLUActivation
                        - MatMulOutput_QK
                        - MatMulOutput_PV
                        # Skip quantizing the layers with the most sensitive activations
                        #- model.layers.21.mlp.down_proj
                        #- model.layers.7.mlp.down_proj
                        #- model.layers.2.mlp.down_proj
                        #- model.layers.8.self_attn.q_proj
                        #- model.layers.8.self_attn.k_proj
                      post_oneshot_calibration: true
                      scheme_overrides:
                        # Enable channelwise quantization for better accuracy
                        Linear:
                          weights:
                            num_bits: 8
                            symmetric: true
                            strategy: channel
                        MatMulLeftInput_QK:
                          input_activations:
                            num_bits: 8
                            symmetric: true
                        # For the embeddings, only weight-quantization makes sense
                        Embedding:
                          input_activations: null
                          weights:
                            num_bits: 8
                            symmetric: false
                """

                sparseml.transformers.oneshot(
                    model=model_path,
                    dataset=ds,
                    recipe=recipe,
                    output_dir=compress_model_path,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Quantize cpu model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--compress-model-path", dest="compress_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--ds", dest="ds", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = quantize_cpu_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Quantize cpu
              model", "outputs": [], "version": "Quantize cpu model@sha256=52864b80d9ab7005b2448f87e67e78bc8a647853403c1af64c5d26b45b70e546"}'
      when:
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: eval-model
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-4.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-cpu-model
    - name: export-model
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --exported-model-path
          - /mnt/models/exported
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def export_model(model_path, exported_model_path):
                from sparseml import export

                export(
                    model_path,
                    task="text-generation",
                    sequence_length=1024,
                    target_path=exported_model_path
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Export model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--exported-model-path", dest="exported_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = export_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Export model",
              "outputs": [], "version": "Export model@sha256=5357e3c10ff9ab32842b255b4b2df5196df8dce067d6b7a9a61e5799f4378fbc"}'
      when:
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-cpu-model
    - name: upload-model
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/exported
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-5.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - export-model
    - name: eval-model-2
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-8.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: export-model-2
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --exported-model-path
          - /mnt/models/exported
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def export_model(model_path, exported_model_path):
                from sparseml import export

                export(
                    model_path,
                    task="text-generation",
                    sequence_length=1024,
                    target_path=exported_model_path
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Export model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--exported-model-path", dest="exported_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = export_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Export model",
              "outputs": [], "version": "Export model@sha256=5357e3c10ff9ab32842b255b4b2df5196df8dce067d6b7a9a61e5799f4378fbc"}'
      when:
      - input: $(tasks.condition-6.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: upload-model-2
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/exported
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-9.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-6.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - export-model-2
    - name: quantize-gpu-model
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --compress-model-path
          - /mnt/models/quant-llm
          - --ds
          - HuggingFaceH4/ultrachat_200k
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq==0.7.1' 'torch==2.2.1' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq==0.7.1'
            'torch==2.2.1' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def quantize_gpu_model(model_path, compress_model_path, ds):
                # Quantizing an LLM
                from transformers import AutoTokenizer
                from datasets import load_dataset

                from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

                MAX_SEQ_LEN = 512
                NUM_EXAMPLES = 512

                def preprocess(example):
                    return {"text": tokenizer.apply_chat_template(example["messages"],
                                                                  tokenize=False)}

                print("Loading the dataset and tokenizers")
                dataset = load_dataset(ds, split="train_sft")
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                ds = dataset.shuffle().select(range(NUM_EXAMPLES))
                ds = ds.map(preprocess)

                examples = [
                    tokenizer(
                        example["text"], padding=False, max_length=MAX_SEQ_LEN,
                        truncation=True,
                    ) for example in ds
                ]

                print("Loaded the dataset and tokenizers")
                print("Starting the quantization")

                # Apply GPTQ
                quantize_config = BaseQuantizeConfig(
                    bits=4,                         # Only support 4 bit
                    group_size=128,                 # Set to g=128 or -1 (for channelwise)
                    desc_act=False,                 # Marlin does not support act_order=True
                    model_file_base_name="model",   # Name of the model.safetensors when we call save_pretrained
                )
                print("Applying GPTQ for quantization")

                model = AutoGPTQForCausalLM.from_pretrained(
                    model_path,
                    quantize_config,
                    device_map="auto")
                model.quantize(examples)

                gptq_save_dir = f"{model_path}-gptq"
                print(f"Saving gptq model to {gptq_save_dir}")
                model.save_pretrained(gptq_save_dir)
                tokenizer.save_pretrained(gptq_save_dir)

                # Convert to Marlin
                print("Reloading in marlin format")
                marlin_model = AutoGPTQForCausalLM.from_quantized(
                    gptq_save_dir,
                    use_marlin=True,
                    device_map="auto")

                print(f"Saving model in marlin format to {compress_model_path}")
                marlin_model.save_pretrained(compress_model_path)
                tokenizer.save_pretrained(compress_model_path)

                print("Quantization process completed")

            import argparse
            _parser = argparse.ArgumentParser(prog='Quantize gpu model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--compress-model-path", dest="compress_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--ds", dest="ds", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = quantize_gpu_model(**_parsed_args)
          image: registry.access.redhat.com/ubi9/python-311
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Quantize gpu
              model", "outputs": [], "version": "Quantize gpu model@sha256=dc6445aa31ba89ff7d4a3c1e6b23f3d7b18df9ed11a72c615e4e9d4ce9fc455d"}'
      when:
      - input: $(tasks.condition-11.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: eval-model-3
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-12.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-11.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-gpu-model
    - name: upload-model-3
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-13.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-11.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-gpu-model
    - name: eval-model-4
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-16.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: upload-model-4
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/sparse-llm
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-17.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - sparse-model
    - name: quantize-cpu-model-2
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --compress-model-path
          - /mnt/models/quant-llm
          - --ds
          - open_platypus
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'datasets' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def quantize_cpu_model(model_path, compress_model_path, ds):
                import sparseml.transformers

                recipe = """
                test_stage:
                  obcq_modifiers:
                    LogarithmicEqualizationModifier:
                      mappings: [
                        [["re:.*q_proj", "re:.*k_proj", "re:.*v_proj"], "re:.*input_layernorm"],
                        [["re:.*gate_proj", "re:.*up_proj"], "re:.*post_attention_layernorm"],
                      ]
                    QuantizationModifier:
                      ignore:
                        # These operations don't make sense to quantize
                        - LlamaRotaryEmbedding
                        - LlamaRMSNorm
                        - SiLUActivation
                        - MatMulOutput_QK
                        - MatMulOutput_PV
                        # Skip quantizing the layers with the most sensitive activations
                        #- model.layers.21.mlp.down_proj
                        #- model.layers.7.mlp.down_proj
                        #- model.layers.2.mlp.down_proj
                        #- model.layers.8.self_attn.q_proj
                        #- model.layers.8.self_attn.k_proj
                      post_oneshot_calibration: true
                      scheme_overrides:
                        # Enable channelwise quantization for better accuracy
                        Linear:
                          weights:
                            num_bits: 8
                            symmetric: true
                            strategy: channel
                        MatMulLeftInput_QK:
                          input_activations:
                            num_bits: 8
                            symmetric: true
                        # For the embeddings, only weight-quantization makes sense
                        Embedding:
                          input_activations: null
                          weights:
                            num_bits: 8
                            symmetric: false
                """

                sparseml.transformers.oneshot(
                    model=model_path,
                    dataset=ds,
                    recipe=recipe,
                    output_dir=compress_model_path,
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Quantize cpu model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--compress-model-path", dest="compress_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--ds", dest="ds", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = quantize_cpu_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Quantize cpu
              model", "outputs": [], "version": "Quantize cpu model@sha256=52864b80d9ab7005b2448f87e67e78bc8a647853403c1af64c5d26b45b70e546"}'
      when:
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: eval-model-5
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-21.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-cpu-model-2
    - name: export-model-3
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --exported-model-path
          - /mnt/models/exported
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def export_model(model_path, exported_model_path):
                from sparseml import export

                export(
                    model_path,
                    task="text-generation",
                    sequence_length=1024,
                    target_path=exported_model_path
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Export model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--exported-model-path", dest="exported_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = export_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Export model",
              "outputs": [], "version": "Export model@sha256=5357e3c10ff9ab32842b255b4b2df5196df8dce067d6b7a9a61e5799f4378fbc"}'
      when:
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-cpu-model-2
    - name: upload-model-5
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/exported
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-22.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - export-model-3
    - name: eval-model-6
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-25.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: export-model-4
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --exported-model-path
          - /mnt/models/exported
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def export_model(model_path, exported_model_path):
                from sparseml import export

                export(
                    model_path,
                    task="text-generation",
                    sequence_length=1024,
                    target_path=exported_model_path
                )

            import argparse
            _parser = argparse.ArgumentParser(prog='Export model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--exported-model-path", dest="exported_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = export_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Export model",
              "outputs": [], "version": "Export model@sha256=5357e3c10ff9ab32842b255b4b2df5196df8dce067d6b7a9a61e5799f4378fbc"}'
      when:
      - input: $(tasks.condition-23.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: upload-model-6
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/exported
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-26.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-23.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - export-model-4
    - name: quantize-gpu-model-2
      params:
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --compress-model-path
          - /mnt/models/quant-llm
          - --ds
          - HuggingFaceH4/ultrachat_200k
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq==0.7.1' 'torch==2.2.1' || PIP_DISABLE_PIP_VERSION_CHECK=1
            python3 -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq==0.7.1'
            'torch==2.2.1' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def quantize_gpu_model(model_path, compress_model_path, ds):
                # Quantizing an LLM
                from transformers import AutoTokenizer
                from datasets import load_dataset

                from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

                MAX_SEQ_LEN = 512
                NUM_EXAMPLES = 512

                def preprocess(example):
                    return {"text": tokenizer.apply_chat_template(example["messages"],
                                                                  tokenize=False)}

                print("Loading the dataset and tokenizers")
                dataset = load_dataset(ds, split="train_sft")
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                ds = dataset.shuffle().select(range(NUM_EXAMPLES))
                ds = ds.map(preprocess)

                examples = [
                    tokenizer(
                        example["text"], padding=False, max_length=MAX_SEQ_LEN,
                        truncation=True,
                    ) for example in ds
                ]

                print("Loaded the dataset and tokenizers")
                print("Starting the quantization")

                # Apply GPTQ
                quantize_config = BaseQuantizeConfig(
                    bits=4,                         # Only support 4 bit
                    group_size=128,                 # Set to g=128 or -1 (for channelwise)
                    desc_act=False,                 # Marlin does not support act_order=True
                    model_file_base_name="model",   # Name of the model.safetensors when we call save_pretrained
                )
                print("Applying GPTQ for quantization")

                model = AutoGPTQForCausalLM.from_pretrained(
                    model_path,
                    quantize_config,
                    device_map="auto")
                model.quantize(examples)

                gptq_save_dir = f"{model_path}-gptq"
                print(f"Saving gptq model to {gptq_save_dir}")
                model.save_pretrained(gptq_save_dir)
                tokenizer.save_pretrained(gptq_save_dir)

                # Convert to Marlin
                print("Reloading in marlin format")
                marlin_model = AutoGPTQForCausalLM.from_quantized(
                    gptq_save_dir,
                    use_marlin=True,
                    device_map="auto")

                print(f"Saving model in marlin format to {compress_model_path}")
                marlin_model.save_pretrained(compress_model_path)
                tokenizer.save_pretrained(compress_model_path)

                print("Quantization process completed")

            import argparse
            _parser = argparse.ArgumentParser(prog='Quantize gpu model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--compress-model-path", dest="compress_model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--ds", dest="ds", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = quantize_gpu_model(**_parsed_args)
          image: registry.access.redhat.com/ubi9/python-311
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Quantize gpu
              model", "outputs": [], "version": "Quantize gpu model@sha256=dc6445aa31ba89ff7d4a3c1e6b23f3d7b18df9ed11a72c615e4e9d4ce9fc455d"}'
      when:
      - input: $(tasks.condition-28.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: eval-model-7
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-29.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-28.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-gpu-model-2
    - name: upload-model-7
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/quant-llm
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-30.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-28.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - quantize-gpu-model-2
    - name: eval-model-8
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-33.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: upload-model-8
      params:
      - name: save_folder_name
        value: $(params.save_folder_name)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --name
          - $(inputs.params.save_folder_name)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def upload_model(model_path, name):
                import os
                from boto3 import client

                print('Starting results upload.')
                s3_endpoint_url = os.environ["s3_host"]
                s3_access_key = os.environ["s3_access_key"]
                s3_secret_key = os.environ["s3_secret_access_key"]
                s3_bucket_name = os.environ["s3_bucket"]

                print(f'Uploading predictions to bucket {s3_bucket_name} '
                      f'to S3 storage at {s3_endpoint_url}')

                s3_client = client(
                    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,
                    aws_secret_access_key=s3_secret_key, verify=False
                )

                # Walk through the local folder and upload files
                for root, dirs, files in os.walk(model_path):
                    for file in files:
                        local_file_path = os.path.join(root, file)
                        #s3_file_path = os.path.join(s3_bucket_name, local_file_path[len(model_path)+1:])
                        s3_file_path = os.path.join(name, local_file_path[len(model_path)+1:])
                        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)
                        print(f'Uploaded {local_file_path}')

                print('Finished uploading results.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Upload model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = upload_model(**_parsed_args)
          env:
          - name: s3_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-models
          - name: s3_secret_access_key
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-models
          - name: s3_host
            valueFrom:
              secretKeyRef:
                key: AWS_S3_ENDPOINT
                name: aws-connection-models
          - name: s3_bucket
            valueFrom:
              secretKeyRef:
                key: AWS_S3_BUCKET
                name: aws-connection-models
          image: registry.access.redhat.com/ubi9/python-311
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: save_folder_name
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Upload model",
              "outputs": [], "version": "Upload model@sha256=9f0185c6243513eba3949cfda601768fc44d79aad60d55b4ba40862ccca42e77"}'
      when:
      - input: $(tasks.condition-34.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: eval-model-9
      params:
      - name: eval_batch_size
        value: $(params.eval_batch_size)
      - name: eval_task
        value: $(params.eval_task)
      - name: shared_volume
        value: $(params.shared_volume)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - /mnt/models/llm
          - --tasks
          - $(inputs.params.eval_task)
          - --batch-size
          - $(inputs.params.eval_batch_size)
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'datasets' 'auto-gptq' 'optimum' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
            -m pip install --quiet --no-warn-script-location 'datasets' 'auto-gptq'
            'optimum' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def eval_model(model_path, tasks, batch_size):
                import subprocess
                import os

                model_args = "pretrained=" + model_path  # + ",trust_remote_code=True"

                # Execute the huggingface_hub-cli command
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                result = subprocess.run(["python", "./lm-evaluation-harness/main.py",
                                         "--model", "sparseml",
                                         "--model_args", model_args,
                                         "--tasks", tasks,
                                         "--batch_size", batch_size,
                                         "--no_cache",
                                         "--write_out",
                                         "--device", "cuda:0",
                                         "--num_fewshot", "0",
                                         "--limit", "1000"],
                                        capture_output=True, text=True, env=env)

                # Check for errors or output
                if result.returncode == 0:
                    print("Model evaluated successfully:")
                    print(result.stdout)
                else:
                    print("Error evaluating the model:")
                    print(result.stderr)

            import argparse
            _parser = argparse.ArgumentParser(prog='Eval model', description='')
            _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--tasks", dest="tasks", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--batch-size", dest="batch_size", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = eval_model(**_parsed_args)
          image: quay.io/ltomasbo/neural-magic:sparseml_eval
          resources:
            limits:
              nvidia.com/gpu: '1'
            requests:
              nvidia.com/gpu: '1'
          volumeMounts:
          - mountPath: /mnt/models
            name: models-shared
        params:
        - name: eval_batch_size
        - name: eval_task
        - name: shared_volume
        volumes:
        - name: models-shared
          persistentVolumeClaim:
            claimName: $(inputs.params.shared_volume)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Eval model",
              "outputs": [], "version": "Eval model@sha256=a186d2e13ef9ab5e6b2a1bf4df6a587645a009d33811e5dd916de5329715af88"}'
      when:
      - input: $(tasks.condition-35.results.outcome)
        operator: in
        values:
        - "true"
      runAfter:
      - download-model
    - name: condition-1
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
    - name: condition-2
      params:
      - name: operand1
        value: $(params.inference_target)
      - name: operand2
        value: CPU
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-3
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-2.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-4
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-5
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-3.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-6
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-2.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-7
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-6.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-8
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-7.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-9
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-6.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-10
      params:
      - name: operand1
        value: $(params.inference_target)
      - name: operand2
        value: GPU
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-11
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-10.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-12
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-11.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-13
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-11.results.outcome)
        operator: in
        values:
        - "true"
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-14
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-10.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-15
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-14.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-16
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-15.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-17
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-14.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-18
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
    - name: condition-19
      params:
      - name: operand1
        value: $(params.inference_target)
      - name: operand2
        value: CPU
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-18.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-20
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-19.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-21
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-22
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-20.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-23
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-19.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-24
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-23.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-25
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-24.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-26
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-23.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-27
      params:
      - name: operand1
        value: $(params.inference_target)
      - name: operand2
        value: GPU
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-18.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-28
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-27.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-29
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-28.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-30
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-28.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-31
      params:
      - name: operand1
        value: $(params.quantize)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-27.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-32
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-31.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-33
      params:
      - name: operand1
        value: $(params.sparse)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-32.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-34
      params:
      - name: operand1
        value: $(params.save_model)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
      when:
      - input: $(tasks.condition-31.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-35
      params:
      - name: operand1
        value: $(params.eval)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi9/python-39
  taskRunSpecs:
  - pipelineTaskName: sparse-model
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: quantize-cpu-model
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-2
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: quantize-gpu-model
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-3
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-4
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: quantize-cpu-model-2
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-5
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-6
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: quantize-gpu-model-2
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-7
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-8
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
  - pipelineTaskName: eval-model-9
    podTemplate:
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
