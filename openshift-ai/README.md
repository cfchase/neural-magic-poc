# How to deploy the pipeline and serve the model

The pipeline is to make use of SparseML to optimize the model, and then
the KServe InferenceService is the one running the DeepSparse runtime with
the model

## SparseML

Create pipeline server, pointing to an S3 bucket

Import the pipeline (``sparseml_pipeline.yaml``) into the pipeline server.
This can be generated by running:

```bash
python pipeline.py
```

### Pipeline Requirements

Cluster storage named ``models-shared``, so that a volume to be shared is created

Data connection, named ``models``, pointing to the S3 bucket to store the resulting model

### Create the images needed for the pipeline

Build the container for the sparsification and the evaluation steps:

```bash
podman build -t quay.io/USER/neural-magic:sparseml -f sparseml_Dockerfile .
podman build -t quay.io/USER/neural-magic:sparseml_eval -f sparseml_eval_Dockerfile .
```

And push them to a registry

```bash
podman push quay.io/USER/neural-magic:sparseml
podman push quay.io/USER/neural-magic:sparseml_eval
```

### Run the pipeline

Run the pipeline selecting the model and the options (evaluation and or sparsification). For now, even though it is given as an option, it is hardcoded to TinyLLama as the recipe applied is hardcoded.

## DeepSparse

Run the optimized model with DeepSparse

### Inference Requirements

Create a secret and a Service Account that points to the S3 endpoint

```bash
oc apply -f secret.yaml
oc apply -f sa.yaml
```

Modified it as needed.

### Create the image needed for the Inference Service

Build the container with:

```bash
podman build -t quay.io/USER/neural-magic:deepsparse -f deepsparse_Dockerfile .
podman build -t quay.io/USER/neural-magic:nm-vllm -f nmvllm_Dockerfile .
```

And push it to a registry

```bash
podman push quay.io/USER/neural-magic:deepsparse
podman push quay.io/USER/neural-magic:nm-vllm
```

### Deploy InferenceService

Note DeepSparse require write access to the mounted volume with the model, so doing a workaround so that it gets copied to an extra mount with `ReadOnly` set to `False`.

```bash
oc apply -f inference.yaml
```

## Testing with Gradio

Run the request.py and access the Gradio server deployed locally at `127.0.0.1:7860`

```bash
python request.py
```
